{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Home","text":"<p><code>LeAB</code> is a Python library for AB testing analysis.</p> <p>Get ready to take a decision !</p> <p><code>LeAB</code> is a Python package designed to streamline A/B test analysis, from raw data to insightful results. It provides a clean API to define experiments, compute metrics, and interpret statistical outcomes with confidence. AB testing has never been more popular, especially on Internet based companies. Even if each test is unique, some questions seem to be asked again and again :</p> <ul> <li>when is my test going to be statistically significant ?</li> <li>is A more successful than B ?</li> <li>does A generate more than B ?</li> </ul> <p>Strong statistical knowledge are required to handle it from start to end correctly. To answer those questions in a simple and robust way, we built le AB. Lets Python do AB testing analysis !</p>"},{"location":"index.html#installation","title":"\ud83d\udee0 Installation","text":"<p><code>LeAB</code> is intended to work with Python 3.12 or above.</p> <p>Installation can be done by using <code>pip</code> :</p> <pre><code>pip install leab\n</code></pre> <p>There are wheels available for Linux, MacOS, and Windows.</p>"},{"location":"index.html#why-leab","title":"\u2728 Why LEAB?","text":"<p>A/B testing is a powerful tool, but often requires stitching together multiple tools and manual steps. LEAB aims to:</p> <ul> <li>\ud83d\udcca Simplify and standardize A/B test workflows</li> <li>\ud83d\udd0d Ensure statistical rigor with built-in checks</li> <li>\ud83e\uddea Support simulations and power analysis</li> </ul> <p>Let the data speak\u2014with confidence.</p> <p>Made with \ud83d\udc99 from Bordeaux \ud83c\udf77 and Montr\u00e9al \ud83c\udf41.</p>"},{"location":"reference/leAverage.html","title":"leAverage","text":""},{"location":"reference/leAverage.html#leaveragepy","title":"leAverage.py","text":""},{"location":"reference/leAverage.html#leab.after.leAverage.leAverage","title":"<code>leAverage</code>","text":"<p>               Bases: <code>TTestSample</code></p> <p>Build leAverage object.</p> <p>Parameters:</p> <pre><code>sample_A (pd.DataFrame): A sample data.\nsample_B (pd.DataFrame): B sample data.\nconfidence_level (float): desired confidence level, default : 95%.\n</code></pre> <p>Example:</p> <pre><code>::\n\n    &gt;&gt;&gt; from leab import after\n    &gt;&gt;&gt; from leab import leDataset\n\n    &gt;&gt;&gt; data = leDataset.SampleLeAverage()\n    &gt;&gt;&gt; ab_test = after.leAverage(data.A, data.B)\n\n    &gt;&gt;&gt; ab_test.sample_A.confidence_interval\n\n    [34.75214007684581, 81.59785992315418]\n\n    &gt;&gt;&gt; ab_test.get_verdict()\n\n    'Sample A mean is greater'\n</code></pre> Source code in <code>leab/after/leAverage.py</code> <pre><code>class leAverage(TTestSample):\n    \"\"\"\n    Build leAverage object.\n\n    Parameters:\n\n        sample_A (pd.DataFrame): A sample data.\n        sample_B (pd.DataFrame): B sample data.\n        confidence_level (float): desired confidence level, default : 95%.\n\n    Example:\n\n        ::\n\n            &gt;&gt;&gt; from leab import after\n            &gt;&gt;&gt; from leab import leDataset\n\n            &gt;&gt;&gt; data = leDataset.SampleLeAverage()\n            &gt;&gt;&gt; ab_test = after.leAverage(data.A, data.B)\n\n            &gt;&gt;&gt; ab_test.sample_A.confidence_interval\n\n            [34.75214007684581, 81.59785992315418]\n\n            &gt;&gt;&gt; ab_test.get_verdict()\n\n            'Sample A mean is greater'\n        \"\"\"\n    def __init__(\n        self,\n        sample_A: pd.DataFrame,\n        sample_B: pd.DataFrame,\n        confidence_level: float = 0.95,\n    ):\n        self.confidence_level = confidence_level\n        self.sample_A = TTestSample(sample_A, confidence_level)\n        self.sample_B = TTestSample(sample_B, confidence_level)\n        self.compute()\n\n    def compute(self) -&gt; None:\n        self._get_diff_mean()\n        self._get_diff_variance()\n        self._get_diff_df()\n        self._get_diff_mean_stddev()\n        self._get_t()\n        self._get_x()\n        self._get_p_value()\n        self._get_d()\n        self._get_SE()\n\n    def _get_diff_mean(self) -&gt; None:\n        self.diff_mean = self.sample_A.mean - self.sample_B.mean\n\n    def _get_diff_variance(self) -&gt; None:\n        self.diff_variance = (\n            self.sample_A.variance / self.sample_A.count_elt\n            + self.sample_B.variance / self.sample_B.count_elt\n        )\n\n    def _get_diff_df(self) -&gt; None:\n        self.diff_df = (\n            self.diff_variance\n            * self.diff_variance\n            / (\n                (self.sample_A.variance / self.sample_A.count_elt)\n                * (self.sample_A.variance / self.sample_A.count_elt)\n                / (self.sample_A.count_elt - 1)\n                + (self.sample_B.variance / self.sample_B.count_elt)\n                * (self.sample_B.variance / self.sample_B.count_elt)\n                / (self.sample_B.count_elt - 1)\n            )\n        )\n\n    def _get_diff_mean_stddev(self) -&gt; None:\n        self.diff_mean_stddev = np.sqrt(self.diff_variance)\n\n    def _get_t(self) -&gt; None:\n        self.t = self.diff_mean / self.diff_mean_stddev\n\n    def _get_x(self) -&gt; None:\n        self.x = self.diff_df / (self.diff_df + self.t * self.t)\n\n    def _get_p_value(self) -&gt; None:\n        self.p_value = sc.betainc(self.diff_df / 2, 0.5, self.x)\n\n    def _get_d(self) -&gt; None:\n        self.d = self.diff_mean\n\n    def _get_SE(self) -&gt; None:\n        self.SE = self.diff_mean_stddev\n\n    def plot_difference_of_means(self):\n        pass\n\n    def get_verdict(self) -&gt; None:\n        if self.p_value &lt; 1 - self.confidence_level:\n            if self.sample_A.mean &gt; self.sample_B.mean:\n                return(\"Sample A mean is greater\")\n            else:\n                return(\"Sample B mean is greater\")\n        else:\n            return(\"No significant difference\")\n</code></pre>"},{"location":"reference/leSample.html","title":"leSample","text":""},{"location":"reference/leSample.html#lesamplepy","title":"leSample.py","text":""},{"location":"reference/leSample.html#leab.before.leSample.leSample","title":"<code>leSample</code>","text":"<p>Build leSample object.</p> <p>Parameters:</p> <pre><code>conversion_rate (float): baseline conversion rate.\nmin_detectable_effect (float): minimum detectable effect.\nsignificance_level (float): alpha, percent of the time a difference will be detected, assuming one does NOT exist.\nstatistical_power (float): 1-beta, percent of the time the minimum effect size will be detected, assuming it exists.\n</code></pre> <p>Example:</p> <pre><code>::\n\n    &gt;&gt;&gt; from leab import before\n\n    &gt;&gt;&gt; ab_test = before.leSample(conversion_rate=20,\n    ...                           min_detectable_effect=2)\n    &gt;&gt;&gt; ab_test.get_size_per_variation()\n    6347\n\n    &gt;&gt;&gt; ab_test.get_duration(avg_daily_total_visitor=1000)\n    13\n</code></pre> Source code in <code>leab/before/leSample.py</code> <pre><code>class leSample:\n    \"\"\"\n    Build leSample object.\n\n    Parameters:\n\n        conversion_rate (float): baseline conversion rate.\n        min_detectable_effect (float): minimum detectable effect.\n        significance_level (float): alpha, percent of the time a difference will be detected, assuming one does NOT exist.\n        statistical_power (float): 1-beta, percent of the time the minimum effect size will be detected, assuming it exists.\n\n    Example:\n\n        ::\n\n            &gt;&gt;&gt; from leab import before\n\n            &gt;&gt;&gt; ab_test = before.leSample(conversion_rate=20,\n            ...                           min_detectable_effect=2)\n            &gt;&gt;&gt; ab_test.get_size_per_variation()\n            6347\n\n            &gt;&gt;&gt; ab_test.get_duration(avg_daily_total_visitor=1000)\n            13\n        \"\"\"\n    def __init__(\n        self,\n        conversion_rate: float,\n        min_detectable_effect: float,\n        significance_level: float = 0.05,\n        statistical_power: float = 0.8,\n        absolute: bool = True,\n    ):\n        self.conversion_rate = conversion_rate / 100\n        self.absolute = absolute\n        self.min_detectable_effect = min_detectable_effect / 100\n        self.absolute_or_relative()\n        self.significance_level = significance_level\n        self.statistical_power = statistical_power\n        self.alpha = significance_level\n        self.beta = 1 - statistical_power\n        self.n = None\n        self.size = self.get_size_per_variation()\n\n    def absolute_or_relative(self) -&gt; None:\n        \"\"\"\n        Set up the min_detectable_effect absolute value or relative to conversion_rate.\n        \"\"\"\n        if self.absolute:\n            self.min_detectable_effect = self.min_detectable_effect\n        else:\n            self.min_detectable_effect = (\n                self.conversion_rate * self.min_detectable_effect\n            )\n\n    @staticmethod\n    def compute_z_score(alpha: float) -&gt; float:\n        \"\"\"\n        Compute z score from alpha value.\n\n        Parameters:\n\n            alpha (float): required alpha value (alpha should already fit the required test).\n\n        Returns:\n\n            Z-score.\n        \"\"\"\n        return norm.ppf(alpha)\n\n    def _get_z_1(self) -&gt; None:\n        self.significance = 1 - (self.alpha / 2)\n        self.z_1 = self.compute_z_score(self.significance)\n\n    def _get_z_2(self) -&gt; None:\n        self.power = 1 - self.beta\n        self.z_2 = self.compute_z_score(self.power)\n\n    def _get_zs(self) -&gt; None:\n        self._get_z_1()\n        self._get_z_2()\n\n    def _get_sd1(self) -&gt; None:\n        \"\"\"\n        Compute standard deviation v1.\n        p-baseline conversion rate which is our estimated p and d-minimum detectable change.\n        \"\"\"\n        self.sd1 = np.sqrt(2 * self.conversion_rate * (1 - self.conversion_rate))\n\n    def _get_sd2(self) -&gt; None:\n        \"\"\"\n        Compute standard deviation v1.\n        p-baseline conversion rate which is our estimated p and d-minimum detectable change.\n        \"\"\"\n        self.sd2 = np.sqrt(\n            self.conversion_rate * (1 - self.conversion_rate)\n            + (self.conversion_rate + self.min_detectable_effect)\n            * (1 - (self.conversion_rate + self.min_detectable_effect))\n        )\n\n    def _get_sds(self) -&gt; None:\n        self._get_sd1()\n        self._get_sd2()\n\n    def _compute_n(self) -&gt; None:\n        self.n = int(\n            np.round(\n                ((self.z_1 * self.sd1 + self.z_2 * self.sd2) ** 2)\n                / (self.min_detectable_effect ** 2)\n            )\n        )\n\n    def get_size_per_variation(self) -&gt; int:\n        \"\"\"\n        Calls all methods used to get the size needed per group to get significance on the test.\n\n        Returns:\n\n            Minimum sample size required per group according to metric denominator.\n        \"\"\"\n        self._get_zs()\n        self._get_sds()\n        self._compute_n()\n        return self.n\n\n    def get_total_size(self) -&gt; int:\n        \"\"\"\n        Calls all methods used to get the total size needed to get significance on the test.\n\n        Returns:\n\n            Minimum total sample size required according to metric denominator.\n        \"\"\"\n        self.total_sample_size = self.n * 2\n        return self.total_sample_size\n\n    def get_duration(self, avg_daily_total_visitor: int, nb_split: int = 2) -&gt; int:\n        \"\"\"\n        Compute the estimate duration in day needed to get significance on the test.\n\n        Parameters:\n\n            avg_daily_total_visitor (int): The first parameter.\n            nb_split (int): The second parameter.\n\n        Returns:\n\n            Return the estimate duration in day needed to get significance on the test.\n        \"\"\"\n        self.avg_daily_total_visitor = avg_daily_total_visitor\n        self.nb_split = nb_split\n        if self.n:\n            self.duration = int(\n                np.round(self.n / (self.avg_daily_total_visitor / self.nb_split))\n            )\n        else:\n            self.get_size_per_variation()\n            self.duration = int(\n                np.round(self.n / (self.avg_daily_total_visitor / self.nb_split))\n            )\n        return self.duration\n</code></pre>"},{"location":"reference/leSample.html#leab.before.leSample.leSample.absolute_or_relative","title":"<code>absolute_or_relative()</code>","text":"<p>Set up the min_detectable_effect absolute value or relative to conversion_rate.</p> Source code in <code>leab/before/leSample.py</code> <pre><code>def absolute_or_relative(self) -&gt; None:\n    \"\"\"\n    Set up the min_detectable_effect absolute value or relative to conversion_rate.\n    \"\"\"\n    if self.absolute:\n        self.min_detectable_effect = self.min_detectable_effect\n    else:\n        self.min_detectable_effect = (\n            self.conversion_rate * self.min_detectable_effect\n        )\n</code></pre>"},{"location":"reference/leSample.html#leab.before.leSample.leSample.compute_z_score","title":"<code>compute_z_score(alpha)</code>  <code>staticmethod</code>","text":"<p>Compute z score from alpha value.</p> <p>Parameters:</p> <pre><code>alpha (float): required alpha value (alpha should already fit the required test).\n</code></pre> <p>Returns:</p> <pre><code>Z-score.\n</code></pre> Source code in <code>leab/before/leSample.py</code> <pre><code>@staticmethod\ndef compute_z_score(alpha: float) -&gt; float:\n    \"\"\"\n    Compute z score from alpha value.\n\n    Parameters:\n\n        alpha (float): required alpha value (alpha should already fit the required test).\n\n    Returns:\n\n        Z-score.\n    \"\"\"\n    return norm.ppf(alpha)\n</code></pre>"},{"location":"reference/leSample.html#leab.before.leSample.leSample.get_duration","title":"<code>get_duration(avg_daily_total_visitor, nb_split=2)</code>","text":"<p>Compute the estimate duration in day needed to get significance on the test.</p> <p>Parameters:</p> <pre><code>avg_daily_total_visitor (int): The first parameter.\nnb_split (int): The second parameter.\n</code></pre> <p>Returns:</p> <pre><code>Return the estimate duration in day needed to get significance on the test.\n</code></pre> Source code in <code>leab/before/leSample.py</code> <pre><code>def get_duration(self, avg_daily_total_visitor: int, nb_split: int = 2) -&gt; int:\n    \"\"\"\n    Compute the estimate duration in day needed to get significance on the test.\n\n    Parameters:\n\n        avg_daily_total_visitor (int): The first parameter.\n        nb_split (int): The second parameter.\n\n    Returns:\n\n        Return the estimate duration in day needed to get significance on the test.\n    \"\"\"\n    self.avg_daily_total_visitor = avg_daily_total_visitor\n    self.nb_split = nb_split\n    if self.n:\n        self.duration = int(\n            np.round(self.n / (self.avg_daily_total_visitor / self.nb_split))\n        )\n    else:\n        self.get_size_per_variation()\n        self.duration = int(\n            np.round(self.n / (self.avg_daily_total_visitor / self.nb_split))\n        )\n    return self.duration\n</code></pre>"},{"location":"reference/leSample.html#leab.before.leSample.leSample.get_size_per_variation","title":"<code>get_size_per_variation()</code>","text":"<p>Calls all methods used to get the size needed per group to get significance on the test.</p> <p>Returns:</p> <pre><code>Minimum sample size required per group according to metric denominator.\n</code></pre> Source code in <code>leab/before/leSample.py</code> <pre><code>def get_size_per_variation(self) -&gt; int:\n    \"\"\"\n    Calls all methods used to get the size needed per group to get significance on the test.\n\n    Returns:\n\n        Minimum sample size required per group according to metric denominator.\n    \"\"\"\n    self._get_zs()\n    self._get_sds()\n    self._compute_n()\n    return self.n\n</code></pre>"},{"location":"reference/leSample.html#leab.before.leSample.leSample.get_total_size","title":"<code>get_total_size()</code>","text":"<p>Calls all methods used to get the total size needed to get significance on the test.</p> <p>Returns:</p> <pre><code>Minimum total sample size required according to metric denominator.\n</code></pre> Source code in <code>leab/before/leSample.py</code> <pre><code>def get_total_size(self) -&gt; int:\n    \"\"\"\n    Calls all methods used to get the total size needed to get significance on the test.\n\n    Returns:\n\n        Minimum total sample size required according to metric denominator.\n    \"\"\"\n    self.total_sample_size = self.n * 2\n    return self.total_sample_size\n</code></pre>"},{"location":"reference/leSuccess.html","title":"leSuccess","text":""},{"location":"reference/leSuccess.html#lesuccesspy","title":"leSuccess.py","text":""},{"location":"reference/leSuccess.html#leab.after.leSuccess.leSuccess","title":"<code>leSuccess</code>","text":"<p>               Bases: <code>Chi2Sample</code></p> <p>Build leSuccess object.</p> <p>Parameters:</p> <pre><code>sample_A (pd.DataFrame): A sample data.\nsample_B (pd.DataFrame): B sample data.\nconfidence_level (float): desired confidence level, default : 95%.\n</code></pre> <p>Example:</p> <pre><code>::\n\n    &gt;&gt;&gt; from leab import leDataset\n    &gt;&gt;&gt; from leab import after\n\n    &gt;&gt;&gt; data = leDataset.SampleLeSuccess()\n    &gt;&gt;&gt; ab_test = after.leSuccess(data.A,\n    ...                           data.B,\n    ...                           confidence_level=0.95)\n    &gt;&gt;&gt; ab_test.sample_A.confidence_interval\n\n    [8.526343659939133, 22.13718821096384]\n\n    &gt;&gt;&gt; ab_test.p_value\n\n    0.25870176105718934\n\n    &gt;&gt;&gt; ab_test.get_verdict()\n\n    'No significant difference'\n</code></pre> Source code in <code>leab/after/leSuccess.py</code> <pre><code>class leSuccess(Chi2Sample):\n    \"\"\"\n    Build leSuccess object.\n\n    Parameters:\n\n        sample_A (pd.DataFrame): A sample data.\n        sample_B (pd.DataFrame): B sample data.\n        confidence_level (float): desired confidence level, default : 95%.\n\n    Example:\n\n        ::\n\n            &gt;&gt;&gt; from leab import leDataset\n            &gt;&gt;&gt; from leab import after\n\n            &gt;&gt;&gt; data = leDataset.SampleLeSuccess()\n            &gt;&gt;&gt; ab_test = after.leSuccess(data.A,\n            ...                           data.B,\n            ...                           confidence_level=0.95)\n            &gt;&gt;&gt; ab_test.sample_A.confidence_interval\n\n            [8.526343659939133, 22.13718821096384]\n\n            &gt;&gt;&gt; ab_test.p_value\n\n            0.25870176105718934\n\n            &gt;&gt;&gt; ab_test.get_verdict()\n\n            'No significant difference'\n        \"\"\"\n    def __init__(\n        self,\n        sample_A: pd.DataFrame,\n        sample_B: pd.DataFrame,\n        confidence_level: float = 0.95,\n    ):\n        self.confidence_level = confidence_level\n        self.sample_A = Chi2Sample(sample_A, self.confidence_level)\n        self.sample_B = Chi2Sample(sample_B, self.confidence_level)\n        self.compute()\n\n    def compute(self) -&gt; None:\n        self._get_contingency_table()\n        self._get_observed_values()\n        self._get_expected_values()\n        self._get_chi_square_statistic()\n        self._get_degree_of_freedom()\n        self._get_p_value()\n\n    def _get_contingency_table(self) -&gt; None:\n        sample_A_value_counts = self.sample_A.sample[\"success\"].value_counts()\n        sample_B_value_counts = self.sample_B.sample[\"success\"].value_counts()\n        self.contingency_table = pd.DataFrame(\n            [sample_A_value_counts, sample_B_value_counts]\n        )\n\n        self.contingency_table.index = [\"sample_A\", \"sample_B\"]\n        self.contingency_table.columns = [\"fail\", \"success\"]\n\n    def _get_observed_values(self) -&gt; None:\n        self.observed_values = self.contingency_table.values\n\n    def _get_expected_values(self) -&gt; None:\n        b = scipy.stats.chi2_contingency(self.contingency_table)\n        self.expected_values = b[3]\n\n    def _get_chi_square_statistic(self) -&gt; None:\n        chi_square = sum(\n            [\n                (o - e) ** 2.0 / e\n                for o, e in zip(self.observed_values, self.expected_values)\n            ]\n        )\n        self.chi_square_statistic = chi_square[0] + chi_square[1]\n\n    def _get_degree_of_freedom(self) -&gt; None:\n        no_of_rows = len(self.contingency_table.iloc[0:2, 0])\n        no_of_columns = len(self.contingency_table.iloc[0, 0:2])\n        self.degree_of_freedom = (no_of_rows - 1) * (no_of_columns - 1)\n\n    def _get_p_value(self) -&gt; None:\n        self.p_value = 1 - chi2.cdf(\n            x=self.chi_square_statistic, df=self.degree_of_freedom\n        )\n\n    def get_verdict(self) -&gt; None:\n        if self.p_value &lt; 1.0 - self.confidence_level:\n            if (\n                self.sample_A.success / self.sample_A.trial\n                &gt; self.sample_B.success / self.sample_B.trial\n            ):\n                return(\"Sample A is more successful\")\n            else:\n                return(\"Sample B is more successful\")\n        else:\n            return(\"No significant difference\")\n</code></pre>"}]}